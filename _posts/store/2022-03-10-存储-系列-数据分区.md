---
layout: post
title: "存储 系列 数据分区（Partitioning）"
subtitle: '原来一个整体的数据，按照一定的规则拆分成多个分片数据'
author: "lichao"
header-img: "img/post/bg/post-bg-2015.jpg"
catalog: true
tags:
  - store
---

分区/分片（Partition/Shard）：原来一个整体的数据，按照一定的规则拆分成多个分片数据。

以数据库表为例：
- 垂直分片：单表10个字段N行，拆分为两个 5字段*N行 的表；
- 水平分片：单表10个字段N行，拆分为两个 10字段*N/2行 的表；

垂直分片和早期微服务拆分思路类似，一个很大的系统，逐渐拆分为多个子系统，原来很多字段的表或者很多表的库，被随之拆分为多个表多个库。

水平分片主要解决单个节点读写负载过高的问题；当拆分为多个分片后，每个节点(机器)可以分别承受若干个分片的负载，从而支持水平扩展（通过加机器数量 而非 增加单机性能 提升读写性能）；拆分后，哪些数据存在哪个节点上，这将具体的分片规则确定了。

垂直分片通常是和业务层的紧关联的，本节余下内容主要指水平分片。

> 这里讨论的分区/分片，在不同的组件对分片有不同的称呼，它对应MongoDB/ElasticSearch等中的 Shard、Kafka中的Partition、RocketMQ中的Queue、HBase 中的Region、Bigtable中的Tablet、Cassandra和Riak中的vnode等。

### 分片策略
#### 简单的轮询
从客户端的角度看，一个简单的负载均衡策略就是轮询：比如四个分片0/1/2/3，第一次请求打到0，第二次打到1、第三次打到2、第四次打到3、第五次又打回0...
**常见的队列（如RocketMQ、Kafka等）在生产者端，写入消息时就是使用这种方式。**
如下图中的TopicA，共有四个分片，每个节点各有两个分片：
![轮询分片策略](/img/post/store/轮询分片策略.png){:height="60%" width="60%"}

> RocketMQ把分片叫队列(Queue)，Kafka则叫分区(Partition)；当支持轮询的负载策略时，往往意味着其他的负载算法也会支持（比如随机、一致性Hash等）

**然而轮询对于一些存储场景并不合适**，比如：
1. 第一次请求：set name_a zhangsan，分片1 记录了KV数据：name_a =zhangsan；
2. 第二/三/四次请求：get name_a，这时轮询到了 分片2/3/4，这时是无法获取到数据的。

我们自然是希望数据A写入成功后，再来找A时，是可以正常找到的（无论是读还是更新），无论下游有几个分区/节点。而此时就可以考虑基于Hash的负载算法了

#### Hash分片
##### Hash取模：基于 Hash 的一个简单实现
1. 第一次请求：set name_a zhangsan，选择的分区为：hash("name_a") % (分区总数)；
2. 第二次请求：get name_a，选择的分区同样为：hash("name_a") % (分区总数)；
这就保证了只要key一样，总是能打到同一个分区(节点)的。
这个Key通常被称之为分片键（Shard Key），它可以是关系型数据库的某个字段、KV数据库的Key、文档数据库中集合的某个字段等，我们将基于这个分片键确定它最终的归属（落到哪个节点）。

![hash分区](/img/post/store/hash分区.png){:height="60%" width="60%"}
下面考虑下扩容的问题，当2个节点扛不住压力的时候，我们需要进行扩容，比如这里扩容到4个节点，我们只需要以分片为维度，将数据迁移到新节点即可（客户端需要更新分片路由）
![hash分片迁移](/img/post/store/hash分片迁移.png){:height="60%" width="60%"}

工程实现上：
- ==MySQL 制定主机实例数（默认3）和分片数（默认101），其实就是一开始有101个库，用三台主机来抗，扩容时，再搞多几台主机，然后把库挂过去即可。==
- ==Redis Cluster：规定16384个分片（slot槽），只有全部分片 都分配了节点，Redis才正常服务；扩缩容时，也会有一个数据迁移的过程（MIGRATE）。==

---

如果四个节点后仍然扛不住负载呢？这时候只能增加分片数量了。
比如增加到5个分片，对应的节点数才能加到5个，这时候的数据迁移就不能以分片维度了，而要具体到所有分片中的记录维度了。比如分片0中的数据，我们将基于它的 分片键 进行迁移：
- ```hash(Key) % 5 == 0``` ：保持原来的位置；
- ```hash(Key) % 5 == 1``` ：迁移至分片1；
- ```hash(Key) % 5 == 2``` ：迁移至分片2；
- ```hash(Key) % 5 == 3``` ：迁移至分片3；
- ```hash(Key) % 5 == 4``` ：迁移至分片4；
同样地，分片1/2/3也要做相同的操作。显然这个迁移的成本会高很多，我们应该尽量避免。

举两个应用场景：
- MySQL早期没有做分片，一个库抗所有；后来规模变大后想做分片，此时会触发上述的迁移动作；
- 现在MySQL默认的分片为101片，假设101个主机实例都扛不住了，那只能再增加分片了，比如提到1000片，然后搞200个主机实例。

--- 

在一些场景下，以缓存系统为例，丢失部分数据是可以容忍的，这时就可以直接丢弃而非迁移了。但是我们希望丢失的数据尽量少。
在Hash取模中，扩容时，比如3个扩到4个，只有 ```hash(key)%3 == hash(key)%4``` 才能打到原来节点，只有1/4的key满足，数据丢失3/4，一致性Hash则可以帮我们尽量减少扩缩容的数据丢失。

##### 优点
这种方式的突出优点是简单性，常用于数据库的分库分表规则。一般采用预分区的方式，提前根据数据量规划好分区数，比如划分为 512 或 1024 张表，保证可支撑未来一段时间的数据容量，再根据负载情况将表 迁移到其他 数据库 中。扩容时通常采用 翻倍扩容，避免 数据映射 全部被 打乱，导致 全量迁移 的情况。
##### 缺点
当 节点数量 变化时，如 扩容 或 收缩 节点，数据节点 映射关系 需要重新计算，会导致数据的 重新迁移。

#### 一致性Hash
假设一个节点就是一个分片，这里简单介绍下一致性Hash：
1. 画一个Hash环，环上有 ```0~2^32-1``` 个点(数)；
2. 确定节点位置：```hash(节点IP+端口) % 2^32``` 得到的数，即节点落在环上的位置；
3. 客户端请求时，```hash("name") % 2^32```，也落在环上了，然后顺时针找到第一个节点即可；

![一致性hash](/img/post/store/一致性hash1.png){:height="60%" width="60%"}

---

假设运气比较好，三个节点分布得比较均匀，一开始各占1/3的负载；
当扩容到第四个节点时，它落在了节点0和1的中间，此时仍然有 2/3 + 1/(3*2) = 5/6 的数据会找回原来的节点，这对于原来的1/4已经是一个很大的进步了；不过此时负载已经不太平衡了。
![一致性hash](/img/post/store/一致性hash2.png){:height="60%" width="60%"}

---

假设运气不太好，节点0/1/2挤在了一起，这个环从一开始就已经不平衡了（Hash倾斜）：
![一致性hash](/img/post/store/一致性hash3.png){:height="60%" width="60%"}

---

**解决这个问题的方式是引入虚拟节点**:
1. 比如真实节点0负责两个虚拟节点：0-0和0-1；节点1/2也类似；
2. 计算虚拟节点hash可以简单地：hash(ip+端口+序号) ，然后落到环上；
3. 客户端请求时，先顺时针找到虚拟节点，然后就能找到真实节点了；
![一致性hash](/img/post/store/一致性hash4.png){:height="60%" width="60%"}
尽管两个虚拟节点仍然有大概率出现倾斜，当有100个、200个，那出现倾斜的概率就大大降低了。

因为虚拟节点近似的随机插入(环)，扩缩容前后很大概率都是均衡的；理想情况下，当3个节点扩容到4个节点时，每个节点的负载是从1/3 变到 1/4的，即只有 1/4的数据丢失。

==虚拟槽分区 巧妙地使用了 哈希空间，使用 分散度良好 的 哈希函数 把所有数据 映射 到一个 固定范围 的 整数集合 中，整数定义为 槽（slot）。这个范围一般 远远大于 节点数，比如 Redis Cluster 槽范围是 0 ~ 16383。槽 是集群内 数据管理 和 迁移 的 基本单位。采用 大范围槽 的主要目的是为了方便 数据拆分 和 集群扩展。每个节点会负责 一定数量的槽。==
![一致性分区](/img/post/store/一致性分区.png){:height="60%" width="60%"}

---

工程实现上：
- 早期基于哨兵的Redis，分片的一种实现方式就是一致性Hash，Redis服务端节点对分片是无感知的，请求都是来之不拒；Redis Cluster通过引入16384个槽（分片），初始时需要指定分片对应的节点。如果节点收到不是自己负责的分片数据的话（hash(key)%16384 = 第几个槽）。
- 一致性Hash是一种常见的负载均衡算法。

##### 优点
一致性哈希 可以很好的解决 **稳定性问题**，可以将所有的 **存储节点** 排列在 首尾 相接 的 Hash 环上，每个 key 在计算 Hash 后会 顺时针找到 临接 的 存储节点 存放。而当有节点 加入 或 退出 时，仅影响该节点在 Hash 环上 顺时针相邻 的 后续节点。

##### 缺点
引入虚拟节点之前，加减节点会造成哈希环中部分数据无法命中。当使用少量节点时，节点变化将大范围影响哈希环中数据映射，不适合少量数据节点的分布式方案。普通的一致性哈希分区在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡。

因为 一致性哈希分区 的这些缺点，一些分布式系统采用 虚拟槽 对 一致性哈希 进行改进。

#### Range分片 